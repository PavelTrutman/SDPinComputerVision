\chapter{Optimization over polynomials}\labelcha{POP}
%TODO

\section{State of the art review}
%TODO

\section{Algebraic preliminaries}
In this whole chapter focused on polynomial optimization and polynomial systems solving, we will follow the notation from \cite{Cox-Little-Shea97}.
Just to keep this chapter self-contained, we will recall some basics of polynomial algebra.

\subsection{The polynomial ring, ideals and varieties}
Firstly, the ring of multivariate polynomials in $n$ variables with coefficients in $\R$ is denoted as $\R[x]$, where $x = \bmB x_1 & x_2 & \cdots & x_n \bmE^\top$.
For $\alpha_1, \alpha_2, \ldots, \alpha_n\in\N$, $x^\alpha$ denotes the monomial ${x_1}^{\alpha_1}\cdot{x_2}^{\alpha_2}\cdot\cdots\cdot{x_n}^{\alpha_n}$, with total degree $|\alpha| = \sum_{i=1}^n \alpha_i$, where $\alpha = \bmB \alpha_1 & \alpha_2 & \cdots & \alpha_n\bmE^\top$.
A polynomial $p\in\R[x]$ can be written as
\begin{align}
  p &= \sum_{\alpha\in\N^n} p_\alpha x^\alpha \labeleq{POP:pal:pol}
\end{align}
with total degree $\deg(p) = \max_{\alpha\in\N^n}|\alpha|$ for non-zero coefficients $p_\alpha\in\R$.

A linear subspace $I \subseteq \R[x]$ is an ideal if $p\in I$ and $q\in\R[x]$ implies $pq \in I$.
Let $f_1, f_2, \ldots, f_m$ be polynomials in $\R[x]$. Then the set
\begin{align}
  \langle f_1, f_2, \ldots, f_m\rangle &= \Bigg\{\sum_{j=1}^mh_jf_j\ |\ h_1, h_2, \ldots, h_m\in\R[x]\Bigg\}
\end{align}
is called the ideal generated by $f_1, f_2, \ldots, f_m$.
Given the ideal $I\in\R[x]$, the algebraic variety of $I$ is the set
\begin{align}
  V_\C(I) &= \big\{x\in\C^n\ |\ f(x) = 0 \text{ for all } f\in I\big\}
\end{align}
and its real variety is
\begin{align}
  V_\R(I) &= V_\C(I) \cap \R^n.
\end{align}
The ideal $I$ is said to be zero-dimensional when its complex variety $V_\C(I)$ is finite.
The vanishing ideal of a subset $V\subseteq\C^n$ is the ideal
\begin{align}
  \Ideal(V) &= \big\{f\in\R[x]\ |\ f(x) = 0 \text{ for all } x\in V\big\}.
\end{align}
The radical ideal of the ideal $I\subseteq \R[x]$ is the ideal
\begin{align}
  \sqrt{I} &= \big\{f\in\R[x]\ |\ f^m\in I \text{ for some } m\in\Z^+\big\}.
\end{align}
The real radical ideal of the ideal $I\subseteq \R[x]$ is the ideal
\begin{align}
  \sqrt[\R]{I} &= \big\{f\in\R[x]\ |\ f^{2m} + \sum_j h_j^2 \in I \text{ for some } h_j\in\R[x], m\in\Z^+\big\}.
\end{align}

The following two theorems are stating the relations between the vanishing and (real) radical ideals.

\begin{theorem}[Hilbert's Nullstellensatz]
  Let $I\in\R[x]$ be an ideal. The radical ideal of $I$ is equal to the vanishing ideal of its variety, i.e.\
  \begin{align}
    \sqrt{I} &= \Ideal\big(V_\C(I)\big).
  \end{align}
\end{theorem}

\begin{theorem}[Real Nullstellensatz]
  Let $I\in\R[x]$ be an ideal. The real radical ideal of $I$ is equal to the vanishing ideal of its real variety, i.e.\
  \begin{align}
    \sqrt[\R]{I} &= \Ideal\big(V_\R(I)\big).
  \end{align}
\end{theorem}

The quotient ring $\R[x]/I$ is the set of all equivalence classes of polynomials in $\R[x]$ for congruence modulo ideal $I$
\begin{align}
  \R[x]/I &= \big\{[f]\ |\ f\in\R[x]\big\},
\end{align}
where the equivalence class $[f]$ is
\begin{align}
  [f] &= \big\{f+g\ |\ q\in I\big\}.
\end{align}
Because $\R[x]/I$ is a ring, it is equipped with addition and multiplication on the equivalence classes:
\begin{align}
  [f] + [g] &= [f + g]\\
  ~[f][g] &= [fg]
\end{align}
for $f, g\in\R[x]$.

For zero-dimensional ideal $I$, there is a relation between the dimension of $\R[x]/I$ and the cardinality of the variety $V_\C(I)$:
\begin{align}
  |V_\C(I)| &\leq \dim\big(\R[x]/I\big).
\end{align}
Moreover, if $I$ is a radical ideal, then
\begin{align}
  |V_\C(I)| &= \dim\big(\R[x]/I\big).
\end{align}

Assume that the number of complex roots is finite and let $N = \dim\big(\R[x]/I\big)$, and therefore $|V_\C(I)|\leq N$.
Consider a set $\Base = \{b_1, b_2, \ldots, b_N\} \subseteq \R[x]$ for which the equivalence classes $[b_1], [b_2], \ldots, [b_N]$ are pairwise distinct and $\big\{[b_1], [b_2], \ldots, [b_N]\big\}$ is a basis of $\R[x]/I$.
Then every polynomial $f\in\R[x]$ can be written in unique way as
\begin{align}
  f &= \sum_{i=1}^Nc_ib_i + p,
\end{align}
where $c_i\in\R$ and $p\in I$.
The normal form of the polynomial $f$ modulo $I$ with respect to the basis $\Base$ is the polynomial
\begin{align}
  \NF_\Base(f) &= \sum_{i=1}^N c_ib_i.
\end{align}

\subsection{Solving systems of polynomial equations by multiplication matrices}
Systems of polynomial equations can be solved by computing eigenvalues and eigenvectors of so called multiplication matrices.
Given $f\in\R[x]$, we define the multiplication operator (by $f$) $\MM_f: \R[x]/I \rightarrow \R[x]/I$ as
\begin{align}
  \MM_f([g]) &= [f][g] = [fg].
\end{align}
It can be shown that $\MM_f$ is a linear mapping, and therefore can be represented by its matrix with respect to the basis $\Base$ of $\R[x]/I$.
For simplicity, we again denote this matrix $\MM_f$ and it is called the multiplication matrix by $f$.
When $\Base = \{b_1, b_2, \ldots, b_N\}$ and we set $\NF_\Base(fb_j) = \sum_{i=1}^N a_{i,j}b_i$ for $a_{ij}\in\R$, then the multiplication matrix is
\begin{align}
  \MM_f &= \bmB a_{1,1} & a_{1,2} & \cdots & a_{1,N}\\
                a_{2,1} & a_{2,2} & \cdots & a_{2,N}\\
                \vdots & \vdots & \ddots & \vdots\\
                a_{N,1} & a_{N,2} & \cdots & a_{N,N}\bmE.
\end{align}

\begin{theorem}[Stickelberger theorem]
  Let $I$ be a zero-dimensional ideal in $\R[x]$, let $\Base = \{b_1, b_2, \dots, b_N\}$ be a basis of $\R[x]/I$, and let $f\in\R[x]$.
  The eigenvalues of the multiplication matrix $\MM_f$ are the evaluations $f(v)$ of the polynomial $f$ at the points $v\in V_\C(I)$.
  Moreover, for all $v\in V_\C(I)$,
  \begin{align}
    (\MM_f)^\top[v]_\Base &= f(v)[v]_\Base,
  \end{align}
  setting $[v]_\Base = \bmB b_1(v) & b_2(v) & \cdots & b_N(v)\bmE^\top$; that is, the vector $[v]_\Base$ is a left eigenvector with eigenvalue $f(v)$ of the multiplication matrix $\MM_f$.
\end{theorem}

Therefore, we can create the multiplication matrix $\MM_{x_i}$ for the variable $x_i$ and then the eigenvalues of $\MM_{x_i}$ correspond to the $x_i$-coordinates of the points $V_\C(I)$.
This means that the solutions of the whole system can be found by computing eigenvalues $\lambda_{x_i} = \big\{\lambda_j(\MM_{x_i})\big\}_{j=1}^N$ of the multiplication matrix $\MM_{x_i}$ for all variables $x_i$.
Then $V_\C(I)$ is a subset of the Cartesian product $\lambda_{x_1} \times \lambda_{x_2} \times \cdots \times \lambda_{x_n}$ and one has to select only the points that are solutions.
However, this method becomes inefficient for large $n$, the number of variables, since $n$ multiplication matrices have to be constructed and their eigenvalues computed.

For this reason, the second property of multiplication matrices is used.
The roots can be recovered from left eigenvectors of $\MM_f$ when all left eigenspaces of $\MM_f$ have dimension one.
This is the case when the values $f(v)$ for $v\in V_\C(I)$ are pairwise distinct and when the ideal $I$ is radical.
In that case, each left eigenvector of $\MM_f$ corresponds to one solution $v\in V_\C(I)$ and the values of the eigenvectors are the evaluations $b_i(v)$ for $b_i\in\Base$, and therefore when the variable $x_i\in\Base$, we can readily obtain its value.

\begin{example}\labelex{POP:mm:ellhyp}
  Let us have a system of two polynomial equations.
  \begin{alignat}{7}
    {}-{}20 & x^2 & {}+{}   & xy & {}-{}12 & y^2 & {}-{}16 & x & {}-{}   & y & {}+{}48 & {}={} & 0 \labeleq{POP:mm:example1}\\
    12      & x^2 & {}-{}58 & xy & {}+{}3  & y^2 & {}+{}46 & x & {}-{}47 & y & {}+{}44 & {}={} & 0 \labeleq{POP:mm:example2}
  \end{alignat}
  The first equation represents an ellipse and the second one a hyperbola as you can see in \reffig{POP:mm:example}.
  Let us solve the system using multiplication matrices.

  \begin{figure}[ht]
    \centering
    \resizebox{0.95\textwidth}{!}{\input{graphs/POP_multiplicationMatrices}}
    \caption{The intersection of the ellipse \refeqb{POP:mm:example1} and the hyperbola \refeqb{POP:mm:example2} with solutions found by the eigenvalue and the eigenvector methods using multiplication matrices.}
    \labelfig{POP:mm:example}
  \end{figure}

  First of all, we have to compute the Gr\"obner basis \cite{Becker93} of the ideal, for example using the $F_4$ Algorithm \cite{F4}.
  We have got the following basis:
  \begin{alignat}{5}
    164 & x^2 & {}+{}99 & y^2 & {}+{}126 & x & {}+{}15  & y & {}-{}404,\\
    41  & xy  & {}+{}3  & y^2 & {}-{}16  & x & {}+{}34  & y & {}-{}52, \\
    41  & y^3 & {}-{}15 & y^2 & {}+{}48  & x & {}-{}170 & y & {}+{}96.
  \end{alignat}
  Now, we can select the monomial basis $\Base$
  \begin{align}
    \Base = \bmB 1 & y & x & y^2 \bmE^\top
  \end{align}
  and construct the multiplication matrices $\MM_x$ and $\MM_y$ accordingly, knowing that
  \begin{alignat}{6}
    \MM_x\Big(\big[1\big]\Big)   & {}={} & \big[x\big]    & {}={} &                     &               & 1                  & \big[x\big], &                     &             &                     &            \\
    \MM_x\Big(\big[y\big]\Big)   & {}={} & \big[xy\big]   & {}={} & {}-{}\frac{3}{41}   & \big[y^2\big] & {}+{}\frac{26}{41} & \big[x\big]  & {}-{}\frac{34}{41}  & \big[y\big] & {}+{}\frac{52}{41}  & \big[1\big],\\
    \MM_x\Big(\big[x\big]\Big)   & {}={} & \big[x^2\big]  & {}={} & {}-{}\frac{99}{164} & \big[y^2\big] & {}-{}\frac{63}{82} & \big[x\big]  & {}-{}\frac{15}{164} & \big[y\big] & {}+{}\frac{101}{41} & \big[1\big],\\
    \MM_x\Big(\big[y^2\big]\Big) & {}={} & \big[xy^2\big] & {}={} & {}-{}\frac{37}{41}  & \big[y^2\big] & {}+{}\frac{20}{41} & \big[x\big]  & {}+{}\frac{18}{41}  & \big[y\big] & {}+{}\frac{40}{41}  & \big[1\big],
  \end{alignat}
  and
  \begin{alignat}{6}
    \MM_y\Big(\big[1\big]\Big)   & {}={} & \big[y\big]   & {}={} &                   &                &                    &             &                   1 & \big[y\big], &                    &            \\
    \MM_y\Big(\big[y\big]\Big)   & {}={} & \big[y^2\big] & {}={} &                 1 & \big[y^2\big], &                    &             &                     &              &                    &            \\
    \MM_y\Big(\big[x\big]\Big)   & {}={} & \big[xy\big]  & {}={} & {}-{}\frac{3}{41} & \big[y^2\big]  & {}+{}\frac{26}{41} & \big[x\big] & {}-{}\frac{34}{41}  & \big[y\big]  & {}+{}\frac{52}{41} & \big[1\big],\\
    \MM_y\Big(\big[y^2\big]\Big) & {}={} & \big[y^3\big] & {}={} & \frac{15}{41}     & \big[y^2\big]  & {}-{}\frac{48}{41} & \big[x\big] & {}+{}\frac{170}{41} & \big[y\big]  & {}-{}\frac{96}{41} & \big[1\big].
  \end{alignat}
  Then, the multiplication matrices are:
  \begin{align}
    \MM_x &= \bmB 0 & \frac{52}{41} & \frac{101}{41} & \frac{40}{41}\\
                  0 & -\frac{34}{41} & -\frac{15}{164} & \frac{18}{41}\\
                  1 & \frac{26}{41} & -\frac{63}{82} & \frac{20}{41}\\
                  0 & -\frac{3}{41} & -\frac{99}{164} & -\frac{37}{41}\bmE,\\
    \MM_y &= \bmB 0 & 0 & \frac{52}{41} & -\frac{96}{41}\\
                  1 & 0 & -\frac{34}{41} & \frac{170}{41}\\
                  0 & 0 & \frac{26}{41} & -\frac{48}{41}\\
                  0 & 1 & -\frac{3}{41} & \frac{15}{41}\bmE.
  \end{align}
  The eigenvalues of $\MM_x$ and $\MM_y$ are
  \begin{align}
    \big\{\lambda_i(\MM_x)\big\}_{i=1}^4 &= \left\{-2;\ -1;\ -\frac{1}{2};\ 1\right\},\\
    \big\{\lambda_i(\MM_y)\big\}_{i=1}^4 &= \{-2;\ 0;\ 1;\ 2\}.
  \end{align}
  Therefore, there are $4\times4=16$ possible solutions to the system and we must verify, which of them are true solutions.
  Let us denote the set of all possible solutions as $\tilde{V}_\C(I)$.
  These possible solutions are shown in \reffig{POP:mm:example} by the blue color.

  Secondly, we compute the left eigenvectors of the multiplication matrix $\MM_x$ such that their first coordinates are ones, as it corresponds to the constant polynomial $b_1 = 1$.
  We obtain following four eigenvectors corresponding to four different solutions:
  \begin{align}
    \bmB 1\\ 1\\ 1\\ 1 \bmE,\
    \bmB 1\\ 0\\ -2\\ 0 \bmE,\
    \bmB 1\\ 2\\ -\frac{1}{2}\\ 4 \bmE,\
    \bmB 1\\ -2\\ -1\\ 4 \bmE.
  \end{align}
  Since the second and the third coordinate corresponds to $b_2 = y$ and $b_3 = x$ respectively, we have got four solutions to the system of polynomials \refeqb{POP:mm:example1} and \refeqb{POP:mm:example2}:
  \begin{align}
    V_\C(I) &= \left\{
        \bmB 1\\ 1 \bmE;\
        \bmB -2\\ 0 \bmE;\
        \bmB -\frac{1}{2}\\ 2 \bmE;\
        \bmB -1\\ -2 \bmE
      \right\}.
  \end{align}
  These solutions are shown by the orange color in \reffig{POP:mm:example}.
\end{example}

\section{Moment matrices}
Polynomial optimization and solving systems of polynomial equations via hierarchies of semidefinite programs is based on the theory of measures and moments.
But to keep the scope simple, we will avoid to introduce this theory.
However, since it provides better understanding of the matter, interested reader may look into \cite{SOS}.
Moreover, we will introduce the only minimal basics to be able to proceed with polynomial optimization and polynomial systems solving.
More and detailed information can be found in \cite{SOS} too.

Now, let us start with the theory about moment matrices, which are crucial for application of SDP on polynomial optimization and polynomial systems solving.
Recall that a polynomial has a form \refeqb{POP:pal:pol}.
Since such a polynomial may have infinite number of coefficients, let us introduce a polynomial $p\in\R[x]$ of the degree $d\in\N$:
\begin{align}
  p(x) &= \sum_{\alpha\in\N_d^n}p_\alpha x^\alpha,
\end{align}
where $\N_d$ are natural numbers (including zero) up to the number $d$.
This polynomial has at most $\binom{n+d}{n}$ non-zero coefficients, since there are $\binom{n+d}{n}$ monomials in $n$ variables up to degree $d$.
We will use the notation $\vc(p)$ for the vector of the coefficients of the polynomial $p$ with respect to some monomial basis $\Base$:
\begin{align}
  \vc(p)\ind{\alpha} &= p_\alpha
\end{align}
for $\alpha\in\N_d^n$.

\begin{definition}[Riesz functional]
  Given a sequence $y\ind\alpha = y_\alpha$ for $\alpha\in\N^n$, we define the Riesz linear functional $\ell_y: \R[x]\rightarrow\R$ such that
  \begin{align}
    \ell_y(x^\alpha) &= y_\alpha
  \end{align}
  for all $\alpha\in\N^n$.
\end{definition}
The linearity of the Riesz functional allows us to apply it on polynomials.
\begin{align}
  \ell_y\big(p(x)\big) = \ell_y\left(\sum_{\alpha\in\N_d^n}p_\alpha x^\alpha\right) = \sum_{\alpha\in\N_d^n}p_\alpha \ell_y(x^\alpha) = \sum_{\alpha\in\N_d^n}p_\alpha y_\alpha
\end{align}
From the equation above, we can see that Riesz functional substitutes a new variable $y_\alpha$ for each monomial $x^\alpha$, and therefore we can interpret the Riesz functional as an operator that linearizes polynomials. 

\begin{example}
  Given polynomial $p\in\R[x_1, x_2]$
  \begin{align}
    p(x) &= x_1^2 + 3x_1x_2 - 7x_2 + 9
  \end{align}
  with $\deg(p) = 2$, the vector of its coefficients with respect to monomial basis
  \begin{align}
    \Base &= \bmB x_1^2 & x_1x_2 & x_2^2 & x_1 & x_2 & 1\bmE^\top
  \end{align}
  is
  \begin{align}
    \vc(p) &= \bmB 1 & 3 & 0 & 0 & -7 & 9\bmE^\top.
  \end{align}
  The Riesz functional of $p(x)$ is
  \begin{align}
    \ell\big(p(x)\big) &= y_{20} + 3y_{11} -7y_{01} + 9y_{00}.
  \end{align}
\end{example}

\begin{definition}[Moment matrix]
  A symmetric matrix $M$ indexed by $\N^n$ is said to be a moment matrix (or generalized Hankel matrix) if its $(\alpha, \beta)$-entry depends only on the sum $\alpha + \beta$ of the indices.
  Given sequence $y\ind\alpha = y_\alpha$ for $\alpha\in\N^n$, the moment matrix $M(y)$ has form
  \begin{align}
    M(y)\ind{\alpha, \beta} &= y_{\alpha + \beta}
  \end{align}
  for $\alpha, \beta \in\N^n$.
\end{definition}

\begin{definition}[Truncated moment matrix]
  Given sequence $y\ind\alpha = y_\alpha$ for $\alpha\in\N^n$, the truncated moment matrix $M_s(y)$ of order $s\in\N$ has form
  \begin{align}
    M_s(y)\ind{\alpha, \beta} &= y_{\alpha + \beta}
  \end{align}
  for $\alpha, \beta \in\N_s^n$.
\end{definition}

The moment matrices are linear in $y$ and symmetric, we can see that
\begin{align}
  M_s(y)\in\Sym^{\binom{n+s}{n}}
\end{align}
since $\binom{n+s}{n}$ is the number of monomials in $n$ variables up to degree $s$.

\begin{example}
  For $n=2$ the moment matrices for different orders are:
  \begin{align}
    M_0(y) &= \bmB y_{00} \bmE,\\
    M_1(y) &= \left[\begin{array}{c:cc}
                y_{00} & y_{10} & y_{01}\\\hdashline
                y_{10} & y_{20} & y_{11}\\
                y_{01} & y_{11} & y_{02}
              \end{array}\right],\\
    M_2(y) &= \left[\begin{array}{c:cc:ccc}
                y_{00} & y_{10} & y_{01} & y_{20} & y_{11} & y_{02}\\\hdashline
                y_{10} & y_{20} & y_{11} & y_{30} & y_{21} & y_{12}\\
                y_{01} & y_{11} & y_{02} & y_{21} & y_{12} & y_{03}\\\hdashline
                y_{20} & y_{30} & y_{21} & y_{40} & y_{31} & y_{22}\\
                y_{11} & y_{21} & y_{12} & y_{31} & y_{22} & y_{13}\\
                y_{02} & y_{12} & y_{03} & y_{22} & y_{13} & y_{04}.
              \end{array}\right]
  \end{align}
  In the blocks separated by the dashed lines all the elements have the same degree.
  Moreover, we can see that the moment matrices of smaller order are nothing more than submatrices of the moment matrices of bigger order.

  And just one example for $n=3$:
  \begin{align}
    M_1(y) &= \left[\begin{array}{c:ccc}
                y_{000} & y_{100} & y_{010} & y_{001}\\\hdashline
                y_{100} & y_{200} & y_{110} & y_{101}\\
                y_{010} & y_{110} & y_{020} & y_{011}\\
                y_{001} & y_{101} & y_{011} & y_{002}
              \end{array}\right].
  \end{align}
\end{example}

\begin{definition}[Localizing matrix]
  Given sequence $y\ind\alpha = y_\alpha$ for $\alpha\in\N^n$ and polynomial $q(x)\in\R[x]$, its localizing matrix $M_s(qy)$ of order $s$ has form
  \begin{align}
    M_s(qy)\ind{\alpha,\beta} &= \sum_\gamma q_\gamma y_{\alpha + \beta + \gamma}
  \end{align}
  for $\alpha, \beta \in\N_s^n$.
\end{definition}

Notation $M_s(qy)$ emphasis that the localizing matrix is bilinear in $q$ and $y$.

\begin{example}
  For $n = 2$ and a polynomial $q(x) = x_1x_2 + 2x_1 + 3$, the localizing matrix is
  \begin{align}
    M_1(qy) &= \left[\begin{array}{c:cc}
                    y_{11} +2y_{10} +3y_{00} & y_{21} +2y_{20} +3y_{10} & y_{12} +2y_{11} +3y_{01}\\\hdashline
                    y_{21} +2y_{20} +3y_{10} & y_{31} +2y_{30} +3y_{20} & y_{22} +2y_{21} +3y_{11}\\
                    y_{12} +2y_{11} +3y_{01} & y_{22} +2y_{21} +3y_{11} & y_{13} +2y_{12} +3y_{02}
               \end{array}\right].
  \end{align}
\end{example}

\section{Polynomial optimization}
The task of the polynomial optimization (POP) is to optimize a polynomial function on a set, which is given by a set of polynomial inequalities.
For given polynomials $p_0, \ldots, p_m \in \R[x]$, we can define a standard polynomial optimization problem in a form \refeqb{POP:opt:polmin}.
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rclrcl@{\hskip0.5cm}l}
    p^* &=& \displaystyle \min_{x\in\R^n} & \multicolumn{3}{l}{p_0(x)} \\
    && \text{s.t.} & p_k(x) &\geq& 0 & (k = 1,\ldots,m)
  \end{array}\labeleq{POP:opt:polmin}
\end{align}
Let the feasibility set $P$ of the optimization problem \refeqb{POP:opt:polmin} be compact (closed and bounded) basic semialgebraic set, defined as
\begin{align}
  P &= \big\{x\in\R^n\ |\ p_k(x)\geq0,\ k = 1,\ldots,m\big\}.
\end{align}
Since the set $P$ is compact, the minimum $p^*$ is attained at a point $x^*\in P$.
On the other hand, we do not assume convexity of neither the polynomial $p_0$ nor the set $P$, and therefore the problem \refeqb{POP:opt:polmin} may have several local minima and several global minima in general case.
We are, of course, interested in the global minimum only.

%TODO History of POP, or in state-of-the art review?

\subsection{Lasserre's LMI hierarchy}
The global minimum can be found by hierarchies of semidefinite programs. This was introduced by J. B. Lasserre in \cite{Lasserre}. He has shown that the polynomial optimization problem \refeqb{POP:opt:polmin} can equivalently written as the following semidefinite program \refeqb{POP:opt:SDP}.
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rclrcl@{\hskip0.5cm}l}
    p^* &=& \displaystyle \inf_{y\in\R^{\N^n}} & \multicolumn{3}{l}{\displaystyle \sum_{\alpha\in\N^n}{p_0}_\alpha y_\alpha} \\
    && \text{s.t.} & y_0 &=& 1\\
    &&& M(y) &\succeq& 0\\
    &&& M(p_ky) &\succeq& 0 & (k = 1,\ldots,m)
  \end{array}\labeleq{POP:opt:SDP}
\end{align}
This infinite-dimensional semidefinite program is not solvable by computers, and therefore consider Lasserre's LMI hierarchy \refeqb{POP:opt:SDPr} for relaxation order $r\in\N$.
\begin{align}
  \arraycolsep=1.4pt
  \begin{array}{rclrcl@{\hskip0.5cm}l}
    p_r^* &=& \displaystyle \inf_{y\in\R^{\N^n_{2r}}} & \multicolumn{2}{l}{\displaystyle \sum_{\alpha\in\N^n_{2r}}{p_0}_\alpha y_\alpha} \\
    && \text{s.t.} & y_0 &=& 1\\
    &&& M_r(y) &\succeq& 0\\
    &&& M_{r-r_k}(p_ky) &\succeq& 0 & (k = 1,\ldots,m)
  \end{array}\labeleq{POP:opt:SDPr}
\end{align}
Where $r_k = \left\lceil\frac{\deg(p_k)}{2}\right\rceil$ and $r\geq\max\{r_1, \ldots, r_m\}$.
The semidefinite program \refeqb{POP:opt:SDPr} is a relaxed version of the program \refeqb{POP:opt:SDP} or of the initial polynomial optimization problem \refeqb{POP:opt:polmin}.

\begin{theorem}[Lassere's LMI hierarchy converges \cite{HenrionLectures}]\labelthe{POP:opt:hierConv}
  For $r\in\N$ holds
  \begin{align}
    p^*_r \leq p^*_{r+1} \leq p^*
  \end{align}
  and
  \begin{align}
    \lim_{r\rightarrow+\infty}p^*_r &= p^*.
  \end{align}
\end{theorem}

The semidefinite program \refeqb{POP:opt:SDPr} can be solved by state of the art semidefinite program solvers or by the Polyopt package as described in \refsec{SDP:impl}.
Solving the relaxed semidefinite programs for increasing relaxation order $r$ gives us tighter and tighter lower bounds on the global minimum of the original problem \refeqb{POP:opt:polmin}.

\begin{theorem}[Generic finite convergence \cite{HenrionLectures}]\labelthe{POP:opt:convergence}
  In the finite-dimensional space of coefficients of polynomials $p_k$, $k = 0, 1, \ldots, m$, defining problem \refeqb{POP:opt:polmin}, there is a low-dimensional algebraic set, which is such that if we choose an instance of the problem \refeqb{POP:opt:polmin} outside of this set, the Lasserre's LMI relaxations have finite convergence, i.e.\ there exists a finite $r^*\in\N$ such that $p^*_r=p^*$ for all $r\in\N: r\geq r^*$.
\end{theorem}

This means that in general it is enough to compute one finite relaxed semidefinite program \refeqb{POP:opt:SDPr} of the relaxation order big enough to obtain the global optimum of the polynomial optimization problem \refeqb{POP:opt:polmin}.
Only in some exceptional and somewhat degenerate problems the finite convergence does not occur and the optimum can not be obtained by computing finite-dimensional semidefinite program in the form \refeqb{POP:opt:SDPr}.

From \refthe{POP:opt:convergence} we know, that the finite convergence of Lasserre's LMI hierarchy is ensured generically for some relaxation order $r$, which is a priory not known to us.
The verification that the finite convergence occurred provides us the following theorem.

\begin{theorem}[Certificate of finite convergence \cite{HenrionLectures}]\labelthe{POP:opt:certConv}
  Let $y^*$ be the solution of the problem \refeqb{POP:opt:SDPr} at a given relaxation order $r > \max\{r_1,\ldots,r_m\}$.
  If
  \begin{align}
    \rank M_{r-\max\{r_1,\ldots,r_m\}}(y^*) &= \rank M_r(y^*)
  \end{align}
  then $p^*_r = p^*$.
\end{theorem}

So when we find relaxation order $r$ big enough for which \refthe{POP:opt:certConv} is satisfied, we know, we are done and we can extract the global optimum.
However, in practise another condition is checked.

\begin{theorem}[Rank-one moment matrix \cite{HenrionLectures}]\labelthe{POP:opt:rank1}
  The condition of \refthe{POP:opt:certConv} is satisfied if
  \begin{align}
    \rank M_r(y^*) &= 1.
  \end{align}
\end{theorem}

If the condition of \refthe{POP:opt:rank1} holds, the global optimum of the problem \refeqb{POP:opt:polmin} can be easily recovered as
\begin{align}
  x^* &= \bmB y_{10\ldots0} & y_{01\ldots0} & \cdots & y_{00\ldots1}\bmE^\top.
\end{align}
 
\begin{example}
  Let us set up some polynomial optimization problem for demonstration purposes.
  We use the same ellipse and hyperbola from \refex{POP:mm:ellhyp} to define us the feasible set, while minimizing the objective function $-x_1-\frac{3}{2}x_2$.
  \begin{align}
    \arraycolsep=1.4pt
    \begin{array}{rclrcl@{\hskip0.5cm}l}
      p^* &=& \displaystyle \min_{x\in\R^2} & \multicolumn{3}{l}{-x_1 -\frac{3}{2}x_2} \\
      && \text{s.t.} & -20x^2 + xy -12y^2 -16x -y +48 &\geq& 0\\
      &&& 12x^2 -58xy +3y^2 +46x -47y +44 &\geq& 0
    \end{array}\labeleq{POP:opt:exPol}
  \end{align}
  We expect that the problem has two global optima attained at
  \begin{align}
    \bmB x^*_1\\x^*_2\bmE &= \left\{\bmB -\frac{1}{2}\\ 2\bmE;\ \bmB 1\\ 1\bmE\right\}
  \end{align}
  with value of the objective function
  \begin{align}
    p^* &=-2.5.
  \end{align}
  The illustration of the problem is depicted in \reffig{POP:opt:example}.

  \begin{figure}[ht]
    \centering
    \resizebox{0.95\textwidth}{!}{\input{graphs/POP_Lasserre}}
    \caption{Feasible region and the expected global minima of the problem \refeqb{POP:opt:exPol}.}
    \labelfig{POP:opt:example}
  \end{figure}

  Firstly, we start with the relaxation order $r = 1$. The relaxed semidefinite problem is following.
  \begin{align}
    \arraycolsep=1.4pt
    \begin{array}{rclrcl@{\hskip0.5cm}l}
      p_1^* &=& \displaystyle \min_{y\in\R^{\N^2_2}} & \multicolumn{3}{l}{-y_{10}-\frac{3}{2}y_{01}} \\
      && \text{s.t.} & y_{00} &=& 1\\
      &&& \bmB y_{00} & y_{10} & y_{01}\\ y_{10} & y_{20} & y_{11}\\ y_{01} & y_{11} & y_{02}\bmE &\succeq& 0\\
      &&& \bmB -20y_{20} +y_{11} -12y_{02} -16y_{10} -y_{01} + 48y_{00}\bmE &\succeq& 0\\
      &&& \bmB 12y_{20} -58y_{11} +3y_{02} +46y_{10} -47y_{01} + 44y_{00}\bmE &\succeq& 0
    \end{array}\labeleq{POP:opt:exSDP1}
  \end{align}
  By solving this problem, we obtain a possible solution
  \begin{align}
    \bmB x_1^*\\ x_2^*\bmE &= \bmB 0.20\\ 1.56\bmE,\\
    p_1^* &= -2.54,
  \end{align}
  which is not feasible.
  The moment matrix has rank
  \begin{align}
    \rank M_1(y^*) &= 2.
  \end{align}
  Since the condition of \refthe{POP:opt:rank1} is not satisfied, we continue with the second relaxation.

  The second relaxation for $r = 2$ is below.
  \begin{align}
    \arraycolsep=1.4pt
    \begin{array}{rclrcl@{\hskip0.5cm}l}
      p_2^* &=& \displaystyle \min_{y\in\R^{\N^2_4}} & \multicolumn{3}{l}{-y_{10}-\frac{3}{2}y_{01}} \\
      && \text{s.t.} & y_{00} &=& 1\\
      &&& \bmB y_{00} & y_{10} & y_{01} & y_{20} & y_{11} & y_{02}\\ y_{10} & y_{20} & y_{11} & y_{30} & y_{21} & y_{12}\\ y_{01} & y_{11} & y_{02} & y_{21} & y_{12} & y_{03} \\ y_{20} & y_{30} & y_{21} & y_{40} & y_{31} & y_{22}\\ y_{11} & y_{21} & y_{12} & y_{31} & y_{22} & y_{13}\\ y_{02} & y_{12} & y_{03} & y_{22} & y_{13} & y_{04}\bmE &\succeq& 0\\
      \multicolumn{4}{r}{\resizebox{11.5cm}{!}{$\bmB -20y_{20} +y_{11} -12y_{02} -16y_{10} -y_{01} + 48y_{00} & -20y_{30} +y_{21} -12y_{12} -16y_{20} -y_{11} + 48y_{10} & -20y_{21} +y_{12} -12y_{03} -16y_{11} -y_{02} + 48y_{01}\\ -20y_{30} +y_{21} -12y_{12} -16y_{20} -y_{11} + 48y_{10} & -20y_{40} +y_{31} -12y_{22} -16y_{30} -y_{21} + 48y_{20} & -20y_{31} +y_{22} -12y_{13} -16y_{21} -y_{12} + 48y_{11}\\ -20y_{21} +y_{12} -12y_{03} -16y_{11} -y_{02} + 48y_{01} & -20y_{31} +y_{22} -12y_{13} -16y_{21} -y_{12} + 48y_{11} & -20y_{22} +y_{13} -12y_{04} -16y_{12} -y_{03} + 48y_{02}\bmE$}} &\succeq& 0\\
      \multicolumn{4}{r}{\resizebox{11.5cm}{!}{$\bmB 12y_{20} -58y_{11} +3y_{02} +46y_{10} -47y_{01} + 44y_{00} & 12y_{30} -58y_{21} +3y_{12} +46y_{20} -47y_{11} + 44y_{10} & 12y_{21} -58y_{12} +3y_{03} +46y_{11} -47y_{02} + 44y_{01}\\ 12y_{30} -58y_{21} +3y_{12} +46y_{20} -47y_{11} + 44y_{10} & 12y_{40} -58y_{31} +3y_{22} +46y_{30} -47y_{21} + 44y_{20} & 12y_{31} -58y_{22} +3y_{13} +46y_{21} -47y_{12} + 44y_{11}\\ 12y_{21} -58y_{12} +3y_{03} +46y_{11} -47y_{02} + 44y_{01} & 12y_{31} -58y_{22} +3y_{13} +46y_{21} -47y_{12} + 44y_{11} & 12y_{22} -58y_{13} +3y_{04} +46y_{12} -47y_{03} + 44y_{02}\bmE$}} &\succeq& 0
    \end{array}\labeleq{POP:opt:exSDP2}
  \end{align}
  This problem has two minima
  \begin{align}
    \bmB x^*_1\\x^*_2\bmE &= \left\{\bmB -\frac{1}{2}\\ 2\bmE;\ \bmB 1\\ 1\bmE\right\},\\
    p^*_2 &= -2.5.
  \end{align}
  They are the same as the minima of the original problem \refeqb{POP:opt:exPol} and it depends on the iterative algorithm and the chosen starting point in which of these minima we end up.
  The moment matrix has rank
  \begin{align}
    \rank M_2(y^*) &= 1,
  \end{align}
  and therefore the global optimum has been found and we do not have to continue with the next relaxation.
  At the end, we verify that \refthe{POP:opt:hierConv} holds.
  \begin{alignat}{2}
    p_1^* \leq{} && p_2^* & \leq p^*\\
    -2.54 \leq{} && -2.5  & \leq -2.5
  \end{alignat}

\end{example}

\section{Solving systems of polynomial equations}
