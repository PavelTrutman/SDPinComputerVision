\chapter{Optimization over polynomials}\labelcha{POP}

\section{State of the art review}

\section{Algebraic preliminaries}
In this whole chapter focused on polynomial optimization and polynomial systems solving, we will follow the notation from \cite{Cox-Little-Shea97}.
Just to keep this chapter self-contained, we will recall some basics of polynomial algebra.

\subsection{The polynomial ring, ideals and varieties}
Firstly, the ring of multivariate polynomials in $n$ variables with coefficients in $\R$ is denoted as $\R[x]$, where $x = \bmB x_1 & x_2 & \cdots & x_n \bmE^\top$.
For $\alpha_1, \alpha_2, \ldots, \alpha_n\in\N$, $x^\alpha$ denotes the monomial ${x_1}^{\alpha_1}\cdot{x_2}^{\alpha_2}\cdots{x_n}^{\alpha_n}$, with total degree $|\alpha| = \sum_{i=1}^n \alpha_i$, where $\alpha = \bmB \alpha_1 & \alpha_2 & \cdots & \alpha_n\bmE^\top$.
A polynomial $p\in\R[x]$ can be written as
\begin{align}
  p &= \sum_{\alpha\in\N^n} p_\alpha x^\alpha
\end{align}
with total degree $\deg(p) = \max_{\alpha\in\N^n}|\alpha|$ for non-zero coefficients $p_\alpha\in\R$.

A linear subspace $I \subseteq \R[x]$ is an ideal if $p\in I$ and $q\in\R[x]$ implies $pq \in I$.
Let $f_1, f_2, \ldots, f_m$ be polynomials in $\R[x]$. Then the set
\begin{align}
  \langle f_1, f_2, \ldots, f_m\rangle &= \Bigg\{\sum_{j=1}^mh_jf_j\ |\ h_1, h_2, \ldots, h_m\in\R[x]\Bigg\}
\end{align}
is called the ideal generated by $f_1, f_2, \ldots, f_m$.
Given the ideal $I\in\R[x]$, the algebraic variety of $I$ is the set
\begin{align}
  V_\C(I) &= \big\{x\in\C^n\ |\ f(x) = 0 \text{ for all } f\in I\big\}
\end{align}
and its real variety is
\begin{align}
  V_\R(I) &= V_\C(I) \cap \R^n.
\end{align}
The ideal $I$ is said to be zero-dimensional when its complex variety $V_\C(I)$ is finite.
The vanishing ideal of a subset $V\subseteq\C^n$ is the ideal
\begin{align}
  \Ideal(V) &= \big\{f\in\R[x]\ |\ f(x) = 0 \text{ for all } x\in V\big\}.
\end{align}
The radical ideal of the ideal $I\subseteq \R[x]$ is the ideal
\begin{align}
  \sqrt{I} &= \big\{f\in\R[x]\ |\ f^m\in I \text{ for some } m\in\Z^+\big\}.
\end{align}
The real radical ideal of the ideal $I\subseteq \R[x]$ is the ideal
\begin{align}
  \sqrt[\R]{I} &= \big\{f\in\R[x]\ |\ f^{2m} + \sum_j h_j^2 \in I \text{ for some } h_j\in\R[x], m\in\Z^+\big\}.
\end{align}

The following two theorems are stating the relations between the vanishing and (real) radical ideals.

\begin{theorem}[Hilbert's Nullstellensatz]
  Let $I\in\R[x]$ be an ideal. The radical ideal of $I$ is equal to the vanishing ideal of its variety, i.e.\
  \begin{align}
    \sqrt{I} &= \Ideal\big(V_\C(I)\big).
  \end{align}
\end{theorem}

\begin{theorem}[Real Nullstellensatz]
  Let $I\in\R[x]$ be an ideal. The real radical ideal of $I$ is equal to the vanishing ideal of its real variety, i.e.\
  \begin{align}
    \sqrt[\R]{I} &= \Ideal\big(V_\R(I)\big).
  \end{align}
\end{theorem}

The quotient ring $\R[x]/I$ is the set of all equivalence classes of polynomials in $\R[x]$ for congruence modulo ideal $I$
\begin{align}
  \R[x]/I &= \big\{[f]\ |\ f\in\R[x]\big\},
\end{align}
where the equivalence class $[f]$ is
\begin{align}
  [f] &= \big\{f+g\ |\ q\in I\big\}.
\end{align}
Because $\R[x]/I$ is a ring, it is equipped with addition and multiplication on the equivalence classes:
\begin{align}
  [f] + [g] &= [f + g]\\
  ~[f][g] &= [fg]
\end{align}
for $f, g\in\R[x]$.

For zero-dimensional ideal $I$, there is a relation between the dimension of $\R[x]/I$ and the cardinality of the variety $V_\C(I)$:
\begin{align}
  |V_\C(I)| &\leq \dim\big(\R[x]/I\big).
\end{align}
Moreover, if $I$ is a radical ideal, then
\begin{align}
  |V_\C(I)| &= \dim\big(\R[x]/I\big).
\end{align}

Assume that the number of complex roots is finite and let $N = \dim\big(\R[x]/I\big)$, and therefore $|V_\C(I)|\leq N$.
Consider a set $\Base = \{b_1, b_2, \ldots, b_N\} \subseteq \R[x]$ for which the equivalence classes $[b_1], [b_2], \ldots, [b_N]$ are pairwise distinct and $\big\{[b_1], [b_2], \ldots, [b_N]\big\}$ is a basis of $\R[x]/I$.
Then every polynomial $f\in\R[x]$ can be written in unique way as
\begin{align}
  f &= \sum_{i=1}^Nc_ib_i + p,
\end{align}
where $c_i\in\R$ and $p\in I$.
The normal form of the polynomial $f$ modulo $I$ with respect to the basis $\Base$ is the polynomial
\begin{align}
  \NF_\Base(f) &= \sum_{i=1}^N c_ib_i.
\end{align}

\subsection{Solving systems of polynomial equations by multiplication matrices}
Systems of polynomial equations can be solved by computing eigenvalues and eigenvectors of so called multiplication matrices.
Given $f\in\R[x]$, we define the multiplication operator (by $f$) $\MM_f: \R[x]/I \rightarrow \R[x]/I$ as
\begin{align}
  \MM_f([g]) &= [f][g] = [fg].
\end{align}
It can be shown that $\MM_f$ is a linear mapping, and therefore can be represented by its matrix with respect to the basis $\Base$ of $\R[x]/I$.
For simplicity, we again denote this matrix $\MM_f$ and it is called the multiplication matrix by $f$.
When $\Base = \{b_1, b_2, \ldots, b_N\}$ and we set $\NF_\Base(fb_j) = \sum_{i=1}^N a_{i,j}b_i$ for $a_{ij}\in\R$, then the multiplication matrix is
\begin{align}
  \MM_f &= \bmB a_{1,1} & a_{1,2} & \cdots & a_{1,N}\\
                a_{2,1} & a_{2,2} & \cdots & a_{2,N}\\
                \vdots & \vdots & \ddots & \vdots\\
                a_{N,1} & a_{N,2} & \cdots & a_{N,N}\bmE.
\end{align}

\begin{theorem}[Stickelberger theorem]
  Let $I$ be a zero-dimensional ideal in $\R[x]$, let $\Base = \{b_1, b_2, \dots, b_N\}$ be a basis of $\R[x]/I$, and let $f\in\R[x]$.
  The eigenvalues of the multiplication matrix $\MM_f$ are the evaluations $f(v)$ of the polynomial $f$ at the points $v\in V_\C(I)$.
  Moreover, for all $v\in V_\C(I)$,
  \begin{align}
    (\MM_f)^\top[v]_\Base &= f(v)[v]_\Base,
  \end{align}
  setting $[v]_\Base = \bmB b_1(v) & b_2(v) & \cdots & b_N(v)\bmE^\top$; that is, the vector $[v]_\Base$ is a left eigenvector with eigenvalue $f(v)$ of the multiplication matrix $\MM_f$.
\end{theorem}

Therefore, we can create the multiplication matrix $\MM_{x_i}$ for the variable $x_i$ and then the eigenvalues of $\MM_{x_i}$ correspond to the $x_i$-coordinates of the points $V_\C(I)$.
This means that the solutions of the whole system can be found by computing eigenvalues $\lambda_{x_i} = \big\{\lambda_j(\MM_{x_i})\big\}_{j=1}^N$ of the multiplication matrix $\MM_{x_i}$ for all variables $x_i$.
Then $V_\C(I)$ is a subset of the Cartesian product $\lambda_{x_1} \times \lambda_{x_2} \times \cdots \times \lambda_{x_n}$ and one has to select only the points that are solutions.
However, this method becomes inefficient for large $n$, the number of variables, since $n$ multiplication matrices have to be constructed and their eigenvalues computed.

For this reason, the second property of multiplication matrices is used.
The roots can be recovered from left eigenvectors of $\MM_f$ when all eigenspaces of $\MM_f^\top$ have dimension one.
This is the case when the values $f(v)$ for $v\in V_\C(I)$ are pairwise distinct and when the ideal $I$ is radical.
In that case, each left eigenvector of $\MM_f$ corresponds to one solution $v\in V_\C(I)$ and the values of the eigenvectors are the evaluations $b_i(v)$ for $b_i\in\Base$, and therefore when the variable $x_i\in\Base$, we can readily obtain its value.

\begin{example}
  Let us have a system of two polynomial equations.
  \begin{alignat}{7}
    {}-{}20 & x^2 & {}+{}   & xy & {}-{}12 & y^2 & {}-{}16 & x & {}-{}   & y & {}+{}48 & {}={} & 0 \labeleq{POP:mm:example1}\\
    12      & x^2 & {}-{}58 & xy & {}+{}3  & y^2 & {}+{}46 & x & {}-{}47 & y & {}+{}44 & {}={} & 0 \labeleq{POP:mm:example2}
  \end{alignat}
  The first equation represents an ellipse and the second one a hyperbola as you can see in \reffig{POP:mm:example}.
  Let us solve the system using multiplication matrices.

  \begin{figure}[ht]
    \centering
    \resizebox{0.95\textwidth}{!}{\input{graphs/POP_multiplicationMatrices}}
    \caption{The intersection of the ellipse \refeqb{POP:mm:example1} and the hyperbola \refeqb{POP:mm:example2} with solutions found by the eigenvalue and the eigenvectors methods using multiplication matrices.}
    \labelfig{POP:mm:example}
  \end{figure}

  First of all, we have to compute the Gr\"obner basis \cite{Becker93} of the ideal, for example using the $F_4$ Algorithm \cite{F4}.
  \begin{alignat}{5}
    164 & x^2 & {}+{}99 & y^2 & {}+{}126 & x & {}+{}15  & y & {}-{}404\\
    41  & xy  & {}+{}3  & y^2 & {}-{}16  & x & {}+{}34  & y & {}-{}52 \\
    41  & y^3 & {}-{}15 & y^2 & {}+{}48  & x & {}-{}170 & y & {}+{}96
  \end{alignat}
  Now, we can select the basis $\Base$
  \begin{align}
    \Base = \bmB 1 & y & x & y^2 \bmE^\top
  \end{align}
  and construct the multiplication matrices $\MM_x$ and $\MM_y$ accordingly, knowing that
  \begin{alignat}{6}
    \MM_x\Big(\big[1\big]\Big)   & {}={} & \big[x\big]    & {}={} &                     &               & 1                  & \big[x\big], &                     &             &                     &            \\
    \MM_x\Big(\big[y\big]\Big)   & {}={} & \big[xy\big]   & {}={} & {}-{}\frac{3}{41}   & \big[y^2\big] & {}+{}\frac{26}{41} & \big[x\big]  & {}-{}\frac{34}{41}  & \big[y\big] & {}+{}\frac{52}{41}  & \big[1\big],\\
    \MM_x\Big(\big[x\big]\Big)   & {}={} & \big[x^2\big]  & {}={} & {}-{}\frac{99}{164} & \big[y^2\big] & {}-{}\frac{63}{82} & \big[x\big]  & {}-{}\frac{15}{164} & \big[y\big] & {}+{}\frac{101}{41} & \big[1\big],\\
    \MM_x\Big(\big[y^2\big]\Big) & {}={} & \big[xy^2\big] & {}={} & {}-{}\frac{37}{41}  & \big[y^2\big] & {}+{}\frac{20}{41} & \big[x\big]  & {}+{}\frac{18}{41}  & \big[y\big] & {}+{}\frac{40}{41}  & \big[1\big],
  \end{alignat}
  and
  \begin{alignat}{6}
    \MM_y\Big(\big[1\big]\Big)   & {}={} & \big[y\big]   & {}={} &                   &                &                    &             &                   1 & \big[y\big], &                    &            \\
    \MM_y\Big(\big[y\big]\Big)   & {}={} & \big[y^2\big] & {}={} &                 1 & \big[y^2\big], &                    &             &                     &              &                    &            \\
    \MM_y\Big(\big[x\big]\Big)   & {}={} & \big[xy\big]  & {}={} & {}-{}\frac{3}{41} & \big[y^2\big]  & {}+{}\frac{26}{41} & \big[x\big] & {}-{}\frac{34}{41}  & \big[y\big]  & {}+{}\frac{52}{41} & \big[1\big],\\
    \MM_y\Big(\big[y^2\big]\Big) & {}={} & \big[y^3\big] & {}={} & \frac{15}{41}     & \big[y^2\big]  & {}-{}\frac{48}{41} & \big[x\big] & {}+{}\frac{170}{41} & \big[y\big]  & {}-{}\frac{96}{41} & \big[1\big].
  \end{alignat}
  Then, the multiplication matrices are:
  \begin{align}
    \MM_x &= \bmB 0 & \frac{52}{41} & \frac{101}{41} & \frac{40}{41}\\
                  0 & -\frac{34}{41} & -\frac{15}{164} & \frac{18}{41}\\
                  1 & \frac{26}{41} & -\frac{63}{82} & \frac{20}{41}\\
                  0 & -\frac{3}{41} & -\frac{99}{164} & -\frac{37}{41}\bmE,\\
    \MM_y &= \bmB 0 & 0 & \frac{52}{41} & -\frac{96}{41}\\
                  1 & 0 & -\frac{34}{41} & \frac{170}{41}\\
                  0 & 0 & \frac{26}{41} & -\frac{48}{41}\\
                  0 & 1 & -\frac{3}{41} & \frac{15}{41}\bmE.
  \end{align}
  The eigenvalues of $\MM_x$ and $\MM_y$ are
  \begin{align}
    \big\{\lambda_i(\MM_x)\big\}_{i=1}^4 &= \left\{-2;\ -1;\ -\frac{1}{2};\ 1\right\},\\
    \big\{\lambda_i(\MM_y)\big\}_{i=1}^4 &= \{-2;\ 0;\ 1;\ 2\}.
  \end{align}
  Therefore, there are $4\times4=16$ possible solutions to the system and we must verify, which of them are true solutions.
  Let us denote the set of all possible solutions as $\tilde{V}_\C(I)$.
  These possible solutions are shown in \reffig{POP:mm:example} by the blue color.

  Secondly, we compute the left eigenvectors of the multiplication matrix $\MM_x$ such that their first coordinates are ones, as it corresponds to the constant polynomial $b_1 = 1$.
  We obtain following four eigenvectors corresponding to four different solutions:
  \begin{align}
    \bmB 1\\ 1\\ 1\\ 1 \bmE,\
    \bmB 1\\ 0\\ -2\\ 0 \bmE,\
    \bmB 1\\ 2\\ -\frac{1}{2}\\ 4 \bmE,\
    \bmB 1\\ -2\\ -1\\ 4 \bmE.
  \end{align}
  Since the second and the third coordinate corresponds to $b_2 = y$ and $b_3 = x$ respectively, we have got four solutions to the system of polynomials \refeqb{POP:mm:example1} and \refeqb{POP:mm:example2}:
  \begin{align}
    V_\C(I) &= \left\{
        \bmB 1\\ 1 \bmE;\
        \bmB -2\\ 0 \bmE;\
        \bmB -\frac{1}{2}\\ 2 \bmE;\
        \bmB -1\\ -2 \bmE
      \right\}.
  \end{align}
  These solutions are shown by the orange color in \reffig{POP:mm:example}.
\end{example}

\section{Moment matrices}

\section{Polynomial optimization}

\section{Solving systems of polynomial equations}
