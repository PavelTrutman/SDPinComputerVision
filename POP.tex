\chapter{Optimization over polynomials}\labelcha{POP}

\section{State of the art review}

\section{Algebraic preliminaries}
In this whole chapter focused on polynomial optimization and polynomial systems solving, we will follow the notation from \cite{Cox-Little-Shea97}.
Just to keep this chapter self-contained, we will recall some basics of polynomial algebra.

\subsection{The polynomial ring, ideals and varieties}
Firstly, the ring of multivariate polynomials in $n$ variables with coefficients in $\R$ is denoted as $\R[x]$, where $x = \bmB x_1 & x_2 & \cdots & x_n \bmE^\top$.
For $\alpha_1, \alpha_2, \ldots, \alpha_n\in\N$, $x^\alpha$ denotes the monomial ${x_1}^{\alpha_1}\cdot{x_2}^{\alpha_2}\cdots{x_n}^{\alpha_n}$, with total degree $|\alpha| = \sum_{i=1}^n \alpha_i$, where $\alpha = \bmB \alpha_1 & \alpha_2 & \cdots & \alpha_n\bmE^\top$.
A polynomial $p\in\R[x]$ can be written as
\begin{eqnarray}
  p = \sum_{\alpha\in\N^n} p_\alpha x^\alpha
\end{eqnarray}
with total degree $\deg(p) = \max_{\alpha\in\N^n}|\alpha|$ for non-zero coefficients $p_\alpha\in\R$.

A linear subspace $I \subseteq \R[x]$ is an ideal if $p\in I$ and $q\in\R[x]$ implies $pq \in I$.
Let $f_1, f_2, \ldots, f_m$ be polynomials in $\R[x]$. Then the set
\begin{eqnarray}
  \langle f_1, f_2, \ldots, f_m\rangle &=& \Bigg\{\sum_{j=1}^mh_jf_j\ |\ h_1, h_2, \ldots, h_m\in\R[x]\Bigg\}
\end{eqnarray}
is called the ideal generated by $f_1, f_2, \ldots, f_m$.
Given the ideal $I\in\R[x]$, the algebraic variety of $I$ is the set
\begin{eqnarray}
  V_\C(I) &=& \big\{x\in\C^n\ |\ f(x) = 0 \text{ for all } f\in I\big\}
\end{eqnarray}
and its real variety is
\begin{eqnarray}
  V_\R(I) &=& V_\C(I) \cap \R^n.
\end{eqnarray}
The ideal $I$ is said to be zero-dimensional when its complex variety $V_\C(I)$ is finite.
The vanishing ideal of a subset $V\subseteq\C^n$ is the ideal
\begin{eqnarray}
  \Ideal(V) &=& \big\{f\in\R[x]\ |\ f(x) = 0 \text{ for all } x\in V\big\}.
\end{eqnarray}
The radical ideal of the ideal $I\subseteq \R[x]$ is the ideal
\begin{eqnarray}
  \sqrt{I} &=& \big\{f\in\R[x]\ |\ f^m\in I \text{ for some } m\in\Z^+\big\}.
\end{eqnarray}
The real radical ideal of the ideal $I\subseteq \R[x]$ is the ideal
\begin{eqnarray}
  \sqrt[\R]{I} &=& \big\{f\in\R[x]\ |\ f^{2m} + \sum_j h_j^2 \in I \text{ for some } h_j\in\R[x], m\in\Z^+\big\}.
\end{eqnarray}

The following two theorems are stating the relations between the vanishing and (real) radical ideals.

\begin{theorem}[Hilbert's Nullstellensatz]
  Let $I\in\R[x]$ be an ideal. The radical ideal of $I$ is equal to the vanishing ideal of its variety, i.e.\
  \begin{eqnarray}
    \sqrt{I} &=& \Ideal\big(V_\C(I)\big).
  \end{eqnarray}
\end{theorem}

\begin{theorem}[Real Nullstellensatz]
  Let $I\in\R[x]$ be an ideal. The real radical ideal of $I$ is equal to the vanishing ideal of its real variety, i.e.\
  \begin{eqnarray}
    \sqrt[\R]{I} &=& \Ideal\big(V_\R(I)\big).
  \end{eqnarray}
\end{theorem}

The quotient ring $\R[x]/I$ is the set of all equivalence classes of polynomials in $\R[x]$ for congruence modulo ideal $I$
\begin{eqnarray}
  \R[x]/I &=& \big\{[f]\ |\ f\in\R[x]\big\},
\end{eqnarray}
where the equivalence class $[f]$ is
\begin{eqnarray}
  [f] &=& \big\{f+g\ |\ q\in I\big\}.
\end{eqnarray}
Because $\R[x]/I$ is a ring, it is equipped with addition and multiplication on the equivalence classes:
\begin{eqnarray}
  [f] + [g] &=& [f + g]\\
  ~[f][g] &=& [fg]
\end{eqnarray}
for $f, g\in\R[x]$.

For zero-dimensional ideal $I$, there is a relation between the dimension of $\R[x]/I$ and the cardinality of the variety $V_\C(I)$:
\begin{eqnarray}
  |V_\C(I)| &\leq& \dim\big(\R[x]/I\big).
\end{eqnarray}
Moreover, if $I$ is a radical ideal, then
\begin{eqnarray}
  |V_\C(I)| &=& \dim\big(\R[x]/I\big).
\end{eqnarray}

Assume that the number of complex roots is finite and let $N = \dim\big(\R[x]/I\big)$, and therefore $|V_\C(I)|\leq N$.
Consider a set $\Base = \{b_1, b_2, \ldots, b_N\} \subseteq \R[x]$ for which the equivalence classes $[b_1], [b_2], \ldots, [b_N]$ are pairwise distinct and $\big\{[b_1], [b_2], \ldots, [b_N]\big\}$ is a basis of $\R[x]/I$.
Then every polynomial $f\in\R[x]$ can be written in unique way as
\begin{eqnarray}
  f &=& \sum_{i=1}^Nc_ib_i + p,
\end{eqnarray}
where $c_i\in\R$ and $p\in I$.
The normal form of the polynomial $f$ modulo $I$ with respect to the basis $\Base$ is the polynomial
\begin{eqnarray}
  \NF_\Base(f) &=& \sum_{i=1}^N c_ib_i.
\end{eqnarray}

\subsection{Solving systems of polynomial equations by multiplication matrices}
Systems of polynomial equations can be solved by computing eigenvalues and eigenvectors of so called multiplication matrices.
Given $f\in\R[x]$, we define the multiplication operator (by $f$) $\MM_f: \R[x]/I \rightarrow \R[x]/I$ as
\begin{eqnarray}
  \MM_f([g]) &=& [f][g]\ =\ [fg].
\end{eqnarray}
It can be shown that $\MM_f$ is a linear mapping, and therefore can be represented by its matrix with respect to the basis $\Base$ of $\R[x]/I$.
For simplicity, we again denote this matrix $\MM_f$ and it is called the multiplication matrix by $f$.
When $\Base = \{b_1, b_2, \ldots, b_N\}$ and we set $\NF_\Base(fb_j) = \sum_{i=1}^N a_{i,j}b_i$ for $a_{ij}\in\R$, then the multiplication matrix is
\begin{eqnarray}
  \MM_f &=& \bmB a_{1,1} & a_{1,2} & \cdots & a_{1,N}\\
                 a_{2,1} & a_{2,2} & \cdots & a_{2,N}\\
                 \vdots & \vdots & \ddots & \vdots\\
                 a_{N,1} & a_{N,2} & \cdots & a_{N,N}\bmE.
\end{eqnarray}

\begin{theorem}[Stickelberger theorem]
  Let $I$ be a zero-dimensional ideal in $\R[x]$, let $\Base = \{b_1, b_2, \dots, b_N\}$ be a basis of $\R[x]/I$, and let $f\in\R[x]$.
  The eigenvalues of the multiplication matrix $\MM_f$ are the evaluations $f(v)$ of the polynomial $f$ at the points $v\in V_\C(I)$.
  Moreover, for all $v\in V_\C(I)$,
  \begin{eqnarray}
    (\MM_f)^\top[v]_\Base &=& f(v)[v]_\Base,
  \end{eqnarray}
  setting $[v]_\Base = \bmB b_1(v) & b_2(v) & \cdots & b_N(v)\bmE^\top$; that is, the vector $[v]_\Base$ is a left eigenvector with eigenvalue $f(v)$ of the multiplication matrix $\MM_f$.
\end{theorem}

Therefore, we can create the multiplication matrix $\MM_{x_i}$ for the variable $x_i$ and then the eigenvalues of $\MM_{x_i}$ correspond to the $x_i$-coordinates of the points $V_\C(I)$.
This means that the solutions of the whole system can be found by computing eigenvalues $\lambda_{x_i} = \big\{\lambda_j(\MM_{x_i})\big\}_{j=1}^N$ of the multiplication matrix $\MM_{x_i}$ for all variables $x_i$.
Then $V_\C(I)$ is a subset of the Cartesian product $\lambda_{x_1} \times \lambda_{x_2} \times \cdots \times \lambda_{x_n}$ and one has to select only the points that are solutions.
However, this method becomes inefficient for large $n$, the number of variables, since $n$ multiplication matrices have to be constructed and their eigenvalues computed.

For this reason, the second property of multiplication matrices is used.
The roots can be recovered from left eigenvectors of $\MM_f$ when all eigenspaces of $\MM_f^\top$ have dimension one.
This is the case when the values $f(v)$ for $v\in V_\C(I)$ are pairwise distinct and when the ideal $I$ is radical.
In that case, each left eigenvector of $\MM_f$ corresponds to one solution $v\in V_\C(I)$ and the values of the eigenvectors are the evaluations $b_i(v)$ for $b_i\in\Base$, and therefore when the variable $x_i\in\Base$, we can readily obtain its value.

\section{Moment matrices}

\section{Polynomial optimization}

\section{Solving systems of polynomial equations}
